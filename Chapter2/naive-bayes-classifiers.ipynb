{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes classifier tend to be faster than algorithms from linear models. But the price for their efficiency is their performance is slightly worse than linear models such as LinearSVC and LogisticRegression.\n",
    "\n",
    "The reason that naive Bayes models are so __efficient__ is that they learn parameters by\n",
    "__looking at each feature individually and collect simple per-class statistics from each\n",
    "feature__.\n",
    "\n",
    "Three kinds of naive bayes classifiers implemented in scikit learn: GaussianNB, MultinomialNB, and BenoulliNB. \n",
    "\n",
    "__GaussianNB__ can be applied to\n",
    "any __continuous data__, while __BernoulliNB assumes binary data__ and __MultinomialNB\n",
    "assumes count data__ (that is, that each feature represents an integer count of something,\n",
    "like how often a word appears in a sentence). BernoulliNB and MultinomialNB\n",
    "are mostly used in text data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BernoulliNB classifier counts how often every feature of each class is not zero.\n",
    "This is most easily understood with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have four data points, with four binary features each. There are two classes,\n",
    "0 and 1. For class 0 (the first and third data points), the first feature is zero two times\n",
    "and nonzero zero times, the second feature is zero one time and nonzero one time,\n",
    "and so on. These same counts are then calculated for the data points in the second\n",
    "class. Counting the nonzero entries per class in essence looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # iterate over each class\n",
    "    # count (sum) entries of 1 per feature\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"Feature counts:\\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly different\n",
    "in what kinds of statistics they compute. __MultinomialNB takes into account the\n",
    "average value of each feature for each class__, while __GaussianNB stores the average value\n",
    "as well as the standard deviation of each feature for each class__.\n",
    "\n",
    "To make a prediction, a data point is compared to the statistics for each of the classes,\n",
    "and the best matching class is predicted. Interestingly, for both MultinomialNB and\n",
    "BernoulliNB, this leads to a prediction formula that is of the same form as in the linear\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths, Weaknesses, and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB and BernoulliNB have a single parameter, __alpha__, which controls\n",
    "model complexity. The way alpha works is that __the algorithm adds to the data alpha\n",
    "many virtual data points that have positive values for all the features__. This results in a\n",
    "“smoothing” of the statistics. __A large alpha means more smoothing, resulting in less\n",
    "complex models__. The algorithm’s performance is relatively robust to the setting of\n",
    "alpha, meaning that __setting alpha is not critical for good performance__. However,\n",
    "tuning it usually improves accuracy somewhat.\n",
    "\n",
    "__GaussianNB is mostly used on very high-dimensional data__, while the __other two variants\n",
    "of naive Bayes are widely used for sparse count data such as text__. MultinomialNB\n",
    "usually performs better than BernoulliNB, particularly on datasets with a relatively\n",
    "large number of nonzero features (i.e., large documents).\n",
    "\n",
    "The naive Bayes models share many of the strengths and weaknesses of the linear\n",
    "models. __They are very fast to train and to predict, and the training procedure is easy\n",
    "to understand__. The models work very well with high-dimensional sparse data and are\n",
    "relatively robust to the parameters. Naive Bayes models are great baseline models and\n",
    "are often used on very large datasets, where training even a linear model might take\n",
    "too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive bayes classifiers algorithms tend to be faster than linear models.\n",
    "- Efficient, looking for individual data and calculate the statistics and collect it.\n",
    "- Gaussian: continuous data.\n",
    "- Multinomial: count data.\n",
    "- Binomial: binary data.\n",
    "- Parameter alpha controls the model complexity. High value of alpha means the data less complex (not so different from linear models for regression).\n",
    "- Strengths: fast train and predict, and no need for parameter tuning.\n",
    "- Gaussian is mostly used for high dimensional data\n",
    "- Multinomial and Binomial are mostly used for text document problem. Multinomial usually faster than Binomial. Good for sparse dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
