{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __scikit-learn Cheat Sheet__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train_test_split: split the dataset into training set and test set\n",
    "    - Parameters:\n",
    "        - X\n",
    "        - y\n",
    "        - random_state\n",
    "        - test_size\n",
    "    - Return: X_train, X_test, y_train, y_test\n",
    "\n",
    "### 2. Scaling \n",
    "    1. StandardScaler: Using mean and standard deviation\n",
    "    2. RobustScaler: Using median and quartiles\n",
    "    3. MinMaxScaler: between 0 and 1\n",
    "    4. Normalizer: project to a circle with radius 1\n",
    "    \n",
    "### 3. Imputation\n",
    "    1. SimpleImputer: fill missing values with something\n",
    "        - Modules: sklearn.impute\n",
    "        - Parameters: \n",
    "            - strategy: 'mean' impute the missing values using the column's mean\n",
    "\n",
    "### 4. LabelEncoder:\n",
    "    - Modules: sklearn.preprocessing\n",
    "    - Ex: \n",
    "        - encoder = LabelEncoder().fit(X_train_categories)\n",
    "        - X_train_encoded = encoder.transform(X_train_categories)\n",
    "\n",
    "### 5. OneHotEncoder:\n",
    "    - Modules: sklearn.preprorcessing\n",
    "    - Ex: \n",
    "        - encoder = OneHotEncoder().fit(X_train_categories)\n",
    "        - X_train_encoded = encoder.transform(X_train_categories)\n",
    "    - Attributes:\n",
    "        - classes_: return the column label of each one hot encoded column\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Supervised Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Classification\n",
    "\n",
    "\n",
    "### 1. k-Nearest Neighbors:\n",
    "    - Idea: Find n_neighbors data points closest to the new data point, take the majority of class and label it by the majority class of the nearest data points\n",
    "    - Modules: sklearn.neighbors\n",
    "    - Class: KNeighborsClassifier\n",
    "    - Parameters:\n",
    "        - n_neighbors: number of neighbors used to classifiy\n",
    "        - weights: \n",
    "            - distance: further points will be less influential \n",
    "            - uniform: no weights used (default)\n",
    "        - metric: minkowski (default)\n",
    "        - p: 1 for manhattan distance, 2 for euclidean distance (default)\n",
    "    - Methods: \n",
    "        - fit, predict\n",
    "        - predict_proba: the probability estimates for test data X\n",
    "    - Attributes:\n",
    "    - Strengths:\n",
    "        - Good for first model to consider\n",
    "        - Fast and easy to understand\n",
    "    - Weaknesses:\n",
    "        - Not good for sparse dataset\n",
    "        - Not a complex model\n",
    "        \n",
    "### 2. Naive Bayes:\n",
    "    - Idea: They learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.\n",
    "    - Modules: sklearn.naive_bayes\n",
    "    - Classes: \n",
    "        - GaussianNB\n",
    "        - MultinommialNB\n",
    "        - BernoulliNB\n",
    "        - ComplementNB\n",
    "    - Parameters:\n",
    "        - alpha (MultinomialNB, BernoulliNB, ComplementNB): control model complexity. large alpha means more smoothing, resulting in less complex model\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - predict_proba: the probability estimates for test data X\n",
    "    - Attributes:\n",
    "        - coef_ (MultinomialNB): as linear model\n",
    "        - intercept_ (MultinomialNB): as linear model\n",
    "        - feature_count_ (MultinomialNB, BernoulliNB, ComplementNB): Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided.\n",
    "    - Strengths:\n",
    "        - Faster than linear models\n",
    "        - Fast to train and to predict\n",
    "        - GaussianNB for continuous data, for highly-dimensional data\n",
    "        - MultinomialNB for count data, for sparse data (text)\n",
    "        - BernouliNB for binomial data, for sparse data (text)\n",
    "        - ComplementNB for imbalanced data\n",
    "    - Weaknesses:\n",
    "        - Worse performance than linear models\n",
    "        \n",
    "### 3. Linear Models\n",
    "    - Idea: seperate the classes using threshold, also with line, plane, or hyperplane\n",
    "    - Modules: \n",
    "        - sklearn.svm: for LinearSVC\n",
    "        - sklearn.linear_model: for LogisticRegression and SGDClassifier\n",
    "    - Classes:\n",
    "        - LogisticRegression\n",
    "        - SGDClassifier\n",
    "        - LinearSVC\n",
    "    - Parameters:\n",
    "        - C: Regularization control parameter, increasing C means less regularization\n",
    "        - penalty: l1 or l2\n",
    "        - muti_class: 'multinomial' will use OnevsAll or Sofmax Regression Classifier\n",
    "        - solver: 'lbfgs' for multinomial parameter\n",
    "        - loss: 'hinge'\n",
    "        - dual (LinearSVC): False, unless there are more features than training instances\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - decision_function: Predict confidence scores for samples.\n",
    "    - Attributes:\n",
    "        - coef_: coefficients of linear function\n",
    "        - intercept: constant value of linear function\n",
    "    - Strengths:\n",
    "        - Fast to train and predict\n",
    "        - Good for high-dimensional data and sparse data\n",
    "    - Weaknesses:\n",
    "        - Not so good on small data\n",
    "        \n",
    "### 4. Decision Trees\n",
    "    - Idea: Using series of tests (hierarchical if/else questions)\n",
    "    - Modules: sklearn.tree\n",
    "    - Classes: DecisionTreeClassifier\n",
    "    - Parameters:\n",
    "        - max_depth: the maximum depth of the tree\n",
    "        - min_samples_split: the mininum number of samples a node must have before it can be split\n",
    "        - min_samples_leaf: the minimum number of samples a leaf node must have\n",
    "        - min_weigt_fraction_leaf: same as min_samples_lead but expressed as a fraction of the total number of weighted instances\n",
    "        - max_leaf_nodes: maxumum number of leaf nodes\n",
    "        - max_features: maximum number of features that are evaluated for splitting at each node.\n",
    "        - Note: incresing min_* or reducing max_* hyperparameters will regularize the model\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - predict_proba\n",
    "    - Attributes:\n",
    "        - feature_importances_: return the feature importances\n",
    "    - Streghts: \n",
    "        - Easily visualized and understood (atleast for small trees).\n",
    "        - Invariant to scalling. Works well without preprocessing the data.\n",
    "    - Weaknesses: \n",
    "        - Tend to overfit and have a worse generalization performance.\n",
    "\n",
    "### 5. Random Forest\n",
    "    - Idea: Bunch of slightly different single Decision Tree \n",
    "    - Modules: sklearn.ensemble\n",
    "    - Classes: RandomForestClassifier\n",
    "    - Parameters:\n",
    "        - n_estimators: larger always better, but more trees need more memory and more time to train.\n",
    "        - max_features: determines how random each tree is and smaller value reduces overfitting.\n",
    "        - max_depth (for preprunning)\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - predict_proba\n",
    "    - Attributes:\n",
    "        - estimators_ : list of DecisionTreeClassifier\n",
    "        - feature_importances_\n",
    "    - Strengths:\n",
    "        - Most widely used machine learning methods.\n",
    "        - Very powerful, often work well without heavy tuning.\n",
    "        - No need for scaling.\n",
    "        - Share benefits from decision tree.\n",
    "    - Weaknesses:\n",
    "        - Large datasets -> time consuming\n",
    "        - Don't tend to perform well on very high dimensional, sparse data (linear model is more appropriate).\n",
    "        - Require more memory and slower to train and to predict than linear models.\n",
    "        \n",
    "### Extra Trees Classifier\n",
    "    - Idea: In each node, the tree is not just finding best threshold, but add more randomness to find the threshold. Adding bias, for lower variance. Faster to train, since the model are not looking for best possible threshold\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: ExtraTreesClassifier\n",
    "        \n",
    "### 6. Gradient Boosting\n",
    "    - Idea: Bunch of decision trees, where each tree tries to correct the previous one\n",
    "    - Modules: sklearn.ensemble\n",
    "    - Classes: GradientBoostingClassifier\n",
    "    - Parameters:\n",
    "        - n_estimators\n",
    "        - learning_rate: control how strong each tree makes correction from previous trees.\n",
    "        - max_depth: small value(1-5) will reduce model complexity.\n",
    "        - max_leaf_nodes\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - predict_proba\n",
    "        - decision_function\n",
    "    - Attributes:\n",
    "        - estimators_\n",
    "        - feature_importances_\n",
    "    - Strenghts:\n",
    "        - Among the most powerful and widely used models for supervised learning.\n",
    "        - Same as other tree implementation algorithm, gbrt works well without scaling.\n",
    "    - Weaknesses:\n",
    "        - Requires careful tuning of parameters and may take a long time to train.\n",
    "        - Doesn't work well on high dimensional sparse data.\n",
    "        \n",
    "### Voting Classifiers\n",
    "    - Idea: Use voting of several estimators\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: VotingClassifier\n",
    "    - Parameters: \n",
    "        - estimators: specified the list of estimators\n",
    "            - ex: estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "        - voting: 'hard' for hard voting\n",
    "        \n",
    "### Bagging Classifier\n",
    "    - Idea: Use baggind and pasting\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: BaggingClassifier\n",
    "    - Parameters: \n",
    "        - model\n",
    "        - n_estimators: number of the same estimators used\n",
    "        - max_samples: number of instances to train\n",
    "        - bootstrap: True for boostraping and False for pasting\n",
    "        - n_jobs: cpu cores, -1 for using all\n",
    "        - bootstrap_features: True for bootstraping features\n",
    "        - max_features: number of features used to train\n",
    "    - Attributes: \n",
    "        - oob_score_: to access out-of-bag evaluation score\n",
    "        - oob_decision_function_: returns the calss probabilities for each training instance\n",
    "        \n",
    "### 7. Support Vector Machines\n",
    "    - Idea: using support vector to classify\n",
    "    - Modules: sklearn.svm\n",
    "    - Classes: SVC\n",
    "    - Parameters:\n",
    "        - C: regularization parameter\n",
    "        - kernel: rbf using Gaussian rbf function, poly using polynomial features\n",
    "        - gamma: paremeter for certain kernel (rbf), increase -> less complex model\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - decision_function\n",
    "    - Attributes:\n",
    "        - support_: indices of support vectors\n",
    "        - support_vectors\n",
    "        - coef_\n",
    "        - intercept_\n",
    "    - Strenghts:\n",
    "        - Powerful model and performs well on a variety of datasets\n",
    "        - Allows complex decision boundaries.\n",
    "        - Works well on high dimensional or low dimensional data.\n",
    "        - Works well if features use the same unit.\n",
    "    - Weaknesses:\n",
    "        - Don't scale very well with the number of samples.\n",
    "        - Running an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\n",
    "        - Require scaling and careful parameters tuning.\n",
    "        - Hard to inspect, difficult to understand why a particular prediction was made.\n",
    "        \n",
    "### AdaBoost Classifier\n",
    "    - Idea: first the base estimator fit the training instances, then the misclassified instance's weights will be updated (increased) to make the model more recognized this missclassified instances. Then the model train it again, and at each iteration the instance's weights will be updated.\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Regression\n",
    "\n",
    "\n",
    "### 1. k-Nearest Neighbors:\n",
    "    - Idea: Find n_neighbors data points closest to the new data point, take the average of the closest data points and label it to new data point\n",
    "    - Modules: sklearn.neighbors\n",
    "    - Classes: KNeighborsRegressor\n",
    "    - Parameters:\n",
    "        - n_neighbors: number of neighbors used to classifiy\n",
    "        - weights: \n",
    "            - distance: further points will be less influential \n",
    "            - uniform: no weights used (default)\n",
    "        - metric: minkowski (default)\n",
    "        - p: 1 for manhattan distance, 2 for euclidean distance (default)\n",
    "    - Methods: \n",
    "        - fit, predict\n",
    "    - Attributes:\n",
    "        - classes_\n",
    "    - Strengths:\n",
    "        - Good for first model to consider\n",
    "        - Fast and easy to understand\n",
    "    - Weaknesses:\n",
    "        - Not good for sparse dataset\n",
    "        - Not a complex model\n",
    "        \n",
    "### 2. Linear Models:\n",
    "    - Idea: Make predictions using linear function of the input features\n",
    "    - Modules: sklearn.linear_model\n",
    "    - Classes: \n",
    "        - LinearRegression: ordinary least squares\n",
    "        - SGDRegressor: Stochastic Gradient Descent\n",
    "        - Ridge: the magnitude of the coefficients must become small, close to zero (L2 Regularization)\n",
    "        - Lasso: makes coefficients for some feature become zero (L1 Regularization)\n",
    "        - ElasticNet\n",
    "        - LinearSVR\n",
    "    - Parameters:\n",
    "        - alpha (Ridge, Lasso, ElasticNet): Regularization control parameter, increasing alpha makes the regularization stronger\n",
    "        - r: ratio of which one is stronger, Ridge or Lasso\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "    - Attributes:\n",
    "        - coef_: coefficients of linear function\n",
    "        - intercept: constant value of linear function\n",
    "        - classes_\n",
    "    - Strengths:\n",
    "        - Fast to train and predict\n",
    "        - Good for high-dimensional data and sparse data\n",
    "    - Weaknesses:\n",
    "        - Not so good on small data\n",
    "\n",
    "### 3. Decision Trees\n",
    "    - Idea: Using series of tests (hierarchical if/else questions)\n",
    "    - Modules: sklearn.tree\n",
    "    - Classes: DecisionTreeRegressor\n",
    "    - Parameters:\n",
    "        - max_depth: the maximum depth of the tree\n",
    "        - min_samples_split: the minimum number of samples required to split an internal node\n",
    "        - min_samples_leaf: the minimum number of samples required to be at a leaf node.\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "    - Attributes:\n",
    "        - feature_importances_: return the feature importances\n",
    "        - classes_\n",
    "    - Streghts: \n",
    "        - Easily visualized and understood (atleast for small trees).\n",
    "        - Invariant to scalling. Works well without preprocessing the data.\n",
    "    - Weaknesses: \n",
    "        - Tend to overfit and have a worse generalization performance.\n",
    "        \n",
    "### 4. Random Forest\n",
    "    - Idea: Bunch of slightly different single Decision Tree \n",
    "    - Modules: sklearn.ensemble\n",
    "    - Classes: RandomForestRegressor\n",
    "    - Parameters:\n",
    "        - n_estimators: larger always better, but more trees need more memory and more time to train.\n",
    "        - max_features: determines how random each tree is and smaller value reduces overfitting.\n",
    "        - max_depth (for preprunning)\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "        - predict_proba\n",
    "    - Attributes:\n",
    "        - estimators_ : list of DecisionTreeClassifier\n",
    "        - feature_importances_\n",
    "        - classes_\n",
    "    - Strengths:\n",
    "        - Most widely used machine learning methods.\n",
    "        - Very powerful, often work well without heavy tuning.\n",
    "        - No need for scaling.\n",
    "        - Share benefits from decision tree.\n",
    "    - Weaknesses:\n",
    "        - Large datasets -> time consuming\n",
    "        - Don't tend to perform well on very high dimensional, sparse data (linear model is more appropriate).\n",
    "        - Require more memory and slower to train and to predict than linear models.\n",
    "        \n",
    "### Extra Trees Regressor\n",
    "    - Idea: In each node, the tree is not just finding best threshold, but add more randomness to find the threshold. Adding bias, for lower variance. Faster to train, since the model are not looking for best possible threshold\n",
    "        \n",
    "### 5. Gradient Boosting\n",
    "    - Idea: Bunch of decision trees, where each tree tries to correct the previous one\n",
    "    - Modules: sklearn.ensemble\n",
    "    - Classes: GradientBoostingRegressor\n",
    "    - Parameters:\n",
    "        - n_estimators\n",
    "        - learning_rate: control how strong each tree makes correction from previous trees.\n",
    "        - max_depth: small value(1-5) will reduce model complexity.\n",
    "        - max_leaf_nodes\n",
    "        - subsample: specifies the fraction of training instances to be used for training each tree\n",
    "        - loss: specifies which cost functions to be used\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "    - Attributes:\n",
    "        - estimators_\n",
    "        - feature_importances_\n",
    "        - classes_\n",
    "    - Strenghts:\n",
    "        - Among the most powerful and widely used models for supervised learning.\n",
    "        - Same as other tree implementation algorithm, gbrt works well without scaling.\n",
    "    - Weaknesses:\n",
    "        - Requires careful tuning of parameters and may take a long time to train.\n",
    "        - Doesn't work well on high dimensional sparse data.\n",
    "        \n",
    "### Voting Regressor\n",
    "    - Idea: Use voting of several estimators\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: VotingRegressor\n",
    "    - Parameters: \n",
    "        - estimators: specified the list of estimators\n",
    "            - ex: estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "        - voting: 'hard' for hard voting\n",
    "        \n",
    "### Bagging Regressor\n",
    "    - Idea: Use baggind and pasting\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: BaggingRegressor\n",
    "    - Parameters: \n",
    "        - model\n",
    "        - n_estimators: number of the same estimators used\n",
    "        - max_samples: number of instances used to train\n",
    "        - bootstrap: True for boostraping and False for pasting\n",
    "        - n_jobs: cpu cores, -1 for using all\n",
    "        - oob_score: If True, we can use the out-of-bag instances (instances that our model never trained on/never be in training samples)\n",
    "        - bootstrap_features: True for bootstraping features\n",
    "        - max_features: number of features used to train\n",
    "    - Attributes: \n",
    "        - oob_score_: to access out-of-bag evaluation score\n",
    "        - oob_decision_function_: returns the calss probabilities for each training instance\n",
    "\n",
    "### 6. Support Vector Machines\n",
    "    - Idea: using support vector to classify\n",
    "    - Modules: sklearn.svm\n",
    "    - Classes: SVC\n",
    "    - Parameters:\n",
    "        - C: regularization parameter\n",
    "        - kernel\n",
    "        - gamma: paremeter for certain kernel\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "    - Attributes:\n",
    "        - support_: indices of support vectors\n",
    "        - support_vectors\n",
    "        - coef_\n",
    "        - intercept_\n",
    "        - classes_\n",
    "    - Strenghts:\n",
    "        - Powerful model and performs well on a variety of datasets\n",
    "        - Allows complex decision boundaries.\n",
    "        - Works well on high dimensional or low dimensional data.\n",
    "        - Works well if features use the same unit.\n",
    "    - Weaknesses:\n",
    "        - Don't scale very well with the number of samples.\n",
    "        - Running an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\n",
    "        - Require scaling and careful parameters tuning.\n",
    "        - Hard to inspect, difficult to understand why a particular prediction was made.\n",
    "        \n",
    "### AdaBoost Regressor\n",
    "    - Idea: first the base estimator fit the training instances, then the misclassified instance's weights will be updated (increased) to make the model more recognized this missclassified instances. Then the model train it again, and at each iteration the instance's weights will be updated.\n",
    "    - Module: sklearn.ensemble\n",
    "    - Class: AdaBoostRegressor\n",
    "\n",
    "### XGBoost: \n",
    "    - Modules: xgboost\n",
    "    - Classes: XGBRegressor\n",
    "    - Parameters:\n",
    "        - n_estimators: how many times to go through the modeling cycle described above, advice: 100-1000\n",
    "        - early_stopping_rounds: the model to stop iterating when the validation score stops improving, advice: 5\n",
    "        - learning_rate\n",
    "        - n_jobs: for parallel processsing, divide the process to several processor\n",
    "    - Methods:\n",
    "        - fit, predict\n",
    "    - Ex: \n",
    "        - model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "        - model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Unsupervised Learning__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Principal Component Analysis\n",
    "    - Idea: find principal component, component with high variance and contain much information\n",
    "    - Modules: sklearn.decomposition\n",
    "    - Classes: PCA\n",
    "    - Parameters:\n",
    "        - n_components: number of principal components\n",
    "        - whitten (boolean)\n",
    "        - svd_solver: 'randomized' to make it RandomizedPCA\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transform\n",
    "        - inverse_transform\n",
    "    - Attributes:\n",
    "        - components_: the principal components\n",
    "        - explained_variance_ratio_: the ratio of variance in each axis\n",
    "        \n",
    "### Incremental PCA\n",
    "    - Idea: just like regular PCA but we have to split the data into mini bathces and then feed it to IPCA one mini-batch at a time.\n",
    "    - Module: sklearn.decomposition\n",
    "    - Class: IncrementalPCA\n",
    "    - Parameters:\n",
    "        - n_components\n",
    "    - Ex: \n",
    "        - n_batches = 100\n",
    "        - inc_pca = IncrementalPCA(n_components=154)\n",
    "        - for X_batch in np.array_split(X_minst, n_batches):\n",
    "            inc_pca.partial_fit(X_batch)\n",
    "        - X_mnist_reduced = inc_pca.transform(X_minst)\n",
    "        \n",
    "### Kernel PCA\n",
    "    - Idea: \n",
    "    - Module: sklearn.decomposition\n",
    "    - Class: KernelPCA\n",
    "    - Parameters:\n",
    "        - n_components\n",
    "        - kernel\n",
    "        - gamma\n",
    "        - fit_inverse_transform: to have inverse_transform method\n",
    "\n",
    "### 2. Non Negative Matrix Factorization\n",
    "    - Idea: finding interesting patterns within the data, for feature extraction\n",
    "    - Modules: sklearn.decomposition\n",
    "    - Classes: NMF\n",
    "    - Parameters: \n",
    "        - n_components:\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transform\n",
    "        - inverse_transform\n",
    "    - Attributes:\n",
    "        - components_\n",
    "\n",
    "### 3. Manifold Learning (t-SNE)\n",
    "    - Idea: find a two-dimensional representation of the data that preserves the distances between points as best as possible, good for visualization\n",
    "    - Modules: sklearn.decomposition\n",
    "    - Classes: TSNE\n",
    "    - Parameters:\n",
    "        - n_components\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transform\n",
    "    - Attributes:\n",
    "    \n",
    "### 4. K-Means Clustering\n",
    "    - Idea: data points that close togerther labeled as the same group\n",
    "    - Modules: sklearn.cluster\n",
    "    - Classes: KMeans\n",
    "    - Parameters:\n",
    "        - n_clusters: number of clusters\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - predict: use it for new data, old data still the same, it does not change the model\n",
    "        - transform: transform X to a cluster-distance space.\n",
    "    - Attributes:\n",
    "        - labels_: label for each training data points\n",
    "        - cluster_centers_: center of each cluster\n",
    "    - Strenghts:\n",
    "        - Can add new features to simple data.\n",
    "        - Easy to understand and to implement.\n",
    "        - Relatively quick.\n",
    "    - Weaknesses:\n",
    "        - Works well only on data with relatively simple shape.\n",
    "        - Assume every directions equally matters.\n",
    "        - Relies on random initialization. \n",
    "        - Restrictive on simple shape.\n",
    "\n",
    "### 5. Agglomerative Clustering\n",
    "    - Idea: : the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satisfied\n",
    "    - Modules: sklearn.cluster\n",
    "    - Classes: AgglomerativeClustering\n",
    "    - Parameters:\n",
    "        - n_clusters:\n",
    "        - linkage: 'ward', 'average', and 'complete'\n",
    "    - Methods:\n",
    "        - fit_predict\n",
    "    - Attributes:\n",
    "        - labels_\n",
    "    - Externals:\n",
    "        - ward (linkage) from scipy.cluster.hierarchy: returns linkage_array\n",
    "        - dendrogram from scipy.cluster.hierarchy: visualize the decision making of Agglomerative Clustering\n",
    "    - Ex, syntax:\n",
    "        - linkage_array = ward(X)\n",
    "        - dendogram(linkage_array)\n",
    "    - Strengths:\n",
    "        - The decision making is easy to understand and visualize\n",
    "    - Weaknesses:\n",
    "        - Can not cluster data with extreme shape\n",
    "\n",
    "### 6. Density Based Spatial Clustering of Application with Noise (DBSCAN)\n",
    "    - Idea: works by identifying points that are in “crowded” regions of the feature space, where many data points are close together.\n",
    "    - Modules: sklearn.cluster\n",
    "    - Classes: DBSCAN\n",
    "    - Parameters:\n",
    "        - min_samples: increasing eps means that more points will be included in a cluster.\n",
    "        - eps: increasing min_samples means that fewer points will be core points, and more points will be labeled as noise.\n",
    "    - Methods:\n",
    "        - fit_predict\n",
    "    - Attributes:\n",
    "    - Strenght:\n",
    "        - We do not have to specify the number of clusters.\n",
    "        - Can capture clusters with complex shapes.\n",
    "        - Can identify points that are not part of any cluster.\n",
    "    - Weaknesses:\n",
    "        - Slower than KMeans and AgglomerativeClustering.\n",
    "        - But still scales to relatively large datasets.\n",
    "        - Complex process.\n",
    "        \n",
    "### Clustering Metrics:\n",
    "    - Adjusted Rand Score\n",
    "    - Sillhouette Score: computes the compactness of a cluster\n",
    "    - Normalized Mutual Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LabelEncoder:\n",
    "    - Modules: sklearn.preprocessing\n",
    "    - Ex: \n",
    "        - encoder = LabelEncoder().fit(X_train_categories)\n",
    "        - X_train_encoded = encoder.transform(X_train_categories)\n",
    "\n",
    "### 2. OneHotEncoder:\n",
    "    - Modules: sklearn.preprorcessing\n",
    "    - Ex: \n",
    "        - encoder = OneHotEncoder().fit(X_train_categories)\n",
    "        - X_train_encoded = encoder.transform(X_train_categories)\n",
    "        \n",
    "### 3. Binning and Discretization\n",
    "    - Idea: convert single feature into several categorical features\n",
    "    - Tools: \n",
    "        - np.linspace\n",
    "        - np.digitize\n",
    "        - ML Algorithm\n",
    "    - Example:\n",
    "        - bins = np.linspace(-3, 3, 11)\n",
    "        - which_bin = np.digitize(X, bins=bins) # each data point in X will be converted to which bin they are belong to\n",
    "        - X_encoded = onehotencoder(sparse=False).fit(which_bin) # assumes onehotencoder has been instantiated\n",
    "\n",
    "### 4. Interactions and Polynomials\n",
    "    - Idea: Creating more complex features\n",
    "    - Ways:\n",
    "        - Combining real dataset with binned dataset\n",
    "        - Using PolynomialFeatures\n",
    "    - PolynomialFeatures:\n",
    "        - Module: sklearn.preprocessing\n",
    "        - Parameters:\n",
    "            - degree\n",
    "            - include_bias (bool)\n",
    "        - Methods:\n",
    "            - fit\n",
    "            - transform\n",
    "            - fit_transform\n",
    "            - get_feature_names\n",
    "        - Attributes:\n",
    "\n",
    "### 5. Feature Selection: Univariate Statistics\n",
    "    - Idea: compute whether there is statistically significant relationship between feature to target vector, then features that are related with the highest confidence are selected\n",
    "    - Modules: sklearn.feature_selection\n",
    "    - Classes: \n",
    "        - SelectPercentile\n",
    "        - SelectKBest\n",
    "    - Parameters: \n",
    "        - percentile (SelectPercentile)\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transform\n",
    "        - get_support\n",
    "    - Attributes:\n",
    "    - Strengths:\n",
    "    - Weaknesses:\n",
    "    \n",
    "### 6. Feature Selection: Model Based\n",
    "    - Idea: uses a supervised machine learning model to judge the importance of each feature, and keeps the important ones.\n",
    "    - Modules: sklearn.feature_selection\n",
    "    - Classes: SelectFromModel\n",
    "    - Parameters:\n",
    "        - ML model\n",
    "        - threshold: specifying how some features are important than others\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transfrom\n",
    "        - get_support\n",
    "    - Attributes:\n",
    "    - Strengths:\n",
    "    - Weaknesses:\n",
    "    \n",
    "### 7. Feature Selection: Iterative (RFE: recursive feature elimination)\n",
    "    - Idea: a series of models are built, with varying number of features\n",
    "    - Modules: sklearn.feature_selection\n",
    "    - Classes: RFE\n",
    "    - Parameters:\n",
    "        - ML model\n",
    "        - n_features_to_select\n",
    "    - Methods:\n",
    "        - fit\n",
    "        - transform\n",
    "        - get_support\n",
    "    - Attributes:\n",
    "    - Strengths:\n",
    "    - Weaknesses:\n",
    "        - Computationaly expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Model Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. cross_val_score\n",
    "    - Module: sklearn.model_selection \n",
    "    - Method: cross_val_score\n",
    "    - Parameters:\n",
    "        - ML model\n",
    "        - X\n",
    "        - y\n",
    "        - cv: number of folds\n",
    "        - scoring\n",
    "    - Control for cross_val_score:\n",
    "        - KFold\n",
    "            - n_splits\n",
    "            - shuffle (bool)\n",
    "        - StratifiedKFold\n",
    "        - LeaveOneOut\n",
    "            - \n",
    "        - ShuffleSplit\n",
    "            - n_splits\n",
    "            - test_size\n",
    "            - train_size\n",
    "        - StratifiedShuffleSplit\n",
    "        - GroupKFold\n",
    "    - Strengths:\n",
    "        - Efficient use of data\n",
    "    - Weaknesses:\n",
    "        - Computationaly more expensive than single split\n",
    "        \n",
    "### 2. cross_val_predict\n",
    "    - Module: sklearn.model_selection \n",
    "    - Method: cross_val_predict\n",
    "    - Paramters:\n",
    "        - ML Model\n",
    "        - X\n",
    "        - y\n",
    "        - cv\n",
    "        \n",
    "### 2. Grid Search\n",
    "    - Module: sklearn.model_selection\n",
    "    - Class: GridSearchCV\n",
    "    - Parameters:\n",
    "        - model\n",
    "        - param_grid\n",
    "        - cv\n",
    "        - scoring\n",
    "    - Methods: (estimator methods)\n",
    "        - fit\n",
    "        - predict\n",
    "        - transform\n",
    "    - Attributes:\n",
    "        - best_params_: stored the best parameter\n",
    "        - best_score_: stored the mean score of cross-validation\n",
    "        - best_estimator_: stored the best estimator\n",
    "        - cv_results_: stored the result of grid search\n",
    "        \n",
    "### 3. Randomized Search\n",
    "    - Module: sklearn.model_selection\n",
    "    - Class: RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluation Metrics and Scoring__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix\n",
    "    - Idea: Matrix with row as true classes and column as predicted classes\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: confusion_matrix\n",
    "    - Parameters: y_test, y_pred\n",
    "    \n",
    "### 2. Precision Score\n",
    "    - Idea: Score of Precision\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: precision_score\n",
    "    - Parameters: y_test, y_pred\n",
    "\n",
    "### 3. Recall Score\n",
    "    - Idea: Score of Recall (sensitivity or true positive rate)\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: recall_score\n",
    "    - Parameters: y_test, y_pred\n",
    "    \n",
    "### 4. f1 Score\n",
    "    - Idea: 2 * (precision*recall)/(precision+recall) \n",
    "    - Module: sklearn.metrics\n",
    "    - Function: f1_score\n",
    "    - Parameters: y_test, y_pred\n",
    "\n",
    "### 5. Classification Report\n",
    "    - Idea: Summary of precision, recall, and f1-score\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: classification_report\n",
    "    - Parameters: y_test, y_pred, target_names\n",
    "    \n",
    "### 6. Precision-Recall Curve \n",
    "    - Idea: To visualize precision-recall trade offs\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: precision_recall_curve\n",
    "    - Parameters: y_test, predict_proba/decision_function\n",
    "    - Returns: precision, recall, thresholds\n",
    "    \n",
    "### 7. Average Precision Score\n",
    "    - Idea: computing the integral or area under the curve of the precision-recall curve\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: average_precision_score\n",
    "    - Parameters: y_test, predict_proba/decision_function\n",
    "    \n",
    "### 8. Receiver Operating Characteristics and AUC\n",
    "    - Idea: False positive rate vs True positive rate\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: roc_curve\n",
    "    - Parameters: y_test, predict_proba/decision_function\n",
    "    - Returns fpr, tpr, thresholds\n",
    "    \n",
    "### 9. ROC and AUC score\n",
    "    - Idea: Summarize the ROC curve using single number\n",
    "    - Module: sklearn.metrics\n",
    "    - Function: roc_auc_score\n",
    "    - Parameters: y_test, predict_proba/decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline\n",
    "\n",
    "### 1. Pipeline: \n",
    "    - Modules: sklearn.pipeline\n",
    "    - Syntax (example): pipeline = Pipeline([('impute', SimpleImputer), ('scalind', MinMaxScaler()), ..., ('model', LinearRegression())])\n",
    "    - Attributes:\n",
    "        - steps\n",
    "        - named_steps: return dictionary with key is the step name\n",
    "\n",
    "### 2. make_pipeline:\n",
    "    - Module: sklearn.pipeline\n",
    "    - Attributes:\n",
    "        - steps\n",
    "        - named_steps: return dictionary with key is the step name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FeatureUnion: create paralel processing, commonly for numerical and categorical features\n",
    "    - Module: sklearn.pipeline\n",
    "    \n",
    "### 4. ColumnTransformer: Bundle together different preprocessing step (ex: bundle together preprocessing step between categorical and numerical features)\n",
    "    - Modules: from sklearn.compose\n",
    "    - Syntax (example): preprocess = ColumnTransformer(transformers=[('num', SimpleImputer(), 'cat', pipeline)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. joblib: to save the model as pickle \n",
    "    - Module: sklearn.externals\n",
    "    - Ex: \n",
    "        - joblib.dump(my_model, 'my_model.pkl')\n",
    "        - my_model_loaded = joblib.load('my_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BaseEstimator\n",
    "    - Module: sklearn.base\n",
    "\n",
    "### 2. TransformerMixin\n",
    "    - Module: sklearn.base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Additional__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One vs One Classifier\n",
    "    - Module: sklearn.multiclass\n",
    "    - Class: OneVsOneClassifier\n",
    "    - Syntax: OneVsOneClassifier(SGDClassifier())\n",
    "    \n",
    "2. One vs Rest classifier\n",
    "    - Module: sklearn.multiclass\n",
    "    - Class: OneVsRestClassifier\n",
    "    - Syntax: OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strach Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_decision_boundary(model, X, y, intensity=0.01):\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, intensity),\n",
    "                         np.arange(y_min, y_max, intensity))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.xlim(X[:, 0].min(), X[:, 0].max())\n",
    "    plt.ylim(X[:, 1].min(), X[:, 1].max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.xlim(0, len(X_train))\n",
    "    plt.ylim(0, 3)\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
